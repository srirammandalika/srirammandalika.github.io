<!DOCTYPE HTML>
<link rel="shortcut icon" type="image/png" href="./images/MLogo.png" />
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Sriram Mandalika</title>

  <meta name="author" content="Sriram Mandalika">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Annapoorna Sai Sriram Mandalika</name>
              </p>
              <p> I am a final-year Undergraduate student at <a href="https://www.srmist.edu.in/">SRM Institute of Science and Technology, Chennai (SRM IST)</a> advised by Dr. <a href="https://www.srmist.edu.in/faculty/dr-athira-m-nambiar/">Athira M. Nambiar</a>, where my focus is vision model agnostics, leaning methodologies and reasoning. <br>

                <br>I enjoy travelling and spending time with loved ones, and I'm open to discussing research or CV system development opportunities—feel free to email me.
           

     
              </p>

               <p style="text-align:center">
                <a href="mailto:mc9991@srmist.edu.in">Email</a> &nbsp/&nbsp
                 <a href="./Data/Sriram_EU_CV.pdf">CV</a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/srirammandalika/">LinkedIn</a> &nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./images/Sriram.png"><img style="width:90%;max-width:90%" alt="profile photo" src="./images/Sriram.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading> Research </heading>
              <p>
                My research interests mainly lie in the areas of computer vision and deep learning. Particularly focused on learning methods for vision models, label-efficient (Semi-Supervised/ Unsupervised /Self-Supervised ) approaches and building agents that can independently explores environment and act in the world by learning from data and self-intuitive interactions<br> 
In addition, I am also interested in classical computer vision areas such as representation learning, domain adaptation and transfer learning.
              </p>
               <p style="text-align:center">
               <a href="https://scholar.google.co.in/citations?hl=en&user=7DQaLdgAAAAJ">Google Scholar</a> &nbsp/&nbsp
               <a href="https://github.com/srirammandalika"> Github </a> 
           </p>
            </td>
          </tr>
        </tbody></table>



	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
        <hr>

        I’ve worked with some amazing people, especially (in no particular order) <a href="https://nambiarathira.blogspot.com/search/label/Home/">Dr. Athira Nambiar</a>, <a href="https://people.iith.ac.in/ckm/">Prof. C. Krishna Mohan</a>, <a href="https://www.linkedin.com/in/mrinmay-sen-ab8ba1176/?originalSubdomain=in">Mr. Mrinmay Sen</a>, <a href="https://www.linkedin.com/in/rahul-biswas-5a2b1a147/">Mr. Rahul Biswas</a>, <a href="https://www.linkedin.com/in/saiveena-suresh-a37705277/?originalSubdomain=in">Ms. Sai Veena Suresh</a>, <a href="https://www.linkedin.com/in/sampath-kumar-a23319251/">Mr. Sampath Kumar</a>, and <a href="https://www.linkedin.com/in/shilpi-garg-399180155/">Ms. Shilpi Garg</a> for supporting my research journey.

	<hr>

	      
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Highlights</heading>
              <ul>
		 <li> Received Outstanding Research Contribution Award 2025, SRM IST</li>     
		 <li> Paper on Precognitive Uncertainty-aware Chain-of-Thought (CoT) for driving scene scenario accepted at <a href="https://cvpr.thecvf.com/"> CVPR 2025 </a>, <a href=""> arxiv</a> (soon)<br>
                 <li> Paper on Semi-Supervised semantic segmentation for driving scene scenario accepted at <a href="https://link.springer.com/chapter/10.1007/978-3-031-78107-0_8"> ICPR 2024 </a>, <a href="https://arxiv.org/abs/2408.04482"> arxiv</a> <br>
                 <li> Book chapter on Speech emotion analysis <a href=https://www.igi-global.com/chapter/deep-learning-based-speech-emotional-analysis-using-convolution-neural-network/347293> IGI Global</a> <br> 
                 <!-- <li> Institute Research Award 2022, <a href=https://www.iitm.ac.in/> IIT Madras </a> <br> -->
                 <li> Student research collaborator at <a href="https://ckmvigil.in/"> IIT Hyderabad</a>, explored Continual Learning and Image Reconstruction methods. <br> 
                 <!-- <li> Paper on Semi-Supervised Domain Adaptation accepted at <a href=https://nips.cc> NeurIPS 2021</a> <br>
                 <li> Paper on Semi-Supervised Action Recognition accepted at <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.<br>
              <li> Paper on Mitigating data Imbalance accepted at <a href="https://eccv2020.eu/">ECCV-W 2020</a>.<br> -->
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
<!--     <p> Selected publications are listed here; for the full list of works, kindly visit the Google Scholar link provided above. </p> -->
		
		  
		<p>For the up-to-date publication list, please visit the <a href="https://scholar.google.co.in/citations?hl=en&user=7DQaLdgAAAAJ">Google Scholar</a> page.</p> <be> 
	
			   
		<div class="pub-note">* Equal contribution.&nbsp;&nbsp;† Equal advising.</div> <br>


		  
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/precog_cvpr_25.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://srirammandalika.github.io/PRIMEDrive-CoT/">
                <papertitle>PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario</papertitle>
              </a>
              <br>
               <strong> Sriram Mandalika </strong>, Lalitha V and Athira Nambiar <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025 <br>
               <p> In this work, we introduce PRIMEDrive-CoT, an uncertainty-aware model for object interaction and Chain-of-Thought (CoT) reasoning in driving scenarios. It integrates LiDAR-based 3D detection with multi-view RGB for interpretable scene understanding. Bayesian GNNs model uncertainty and interactions, while CoT reasoning and Grad-CAM enhance interpretability. Evaluated on DriveCoT, PRIMEDrive-CoT outperforms existing CoT and risk-aware models.</p>
              </td>
          </tr> </tbody></table>

		  
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/XAL_Cityscape_new.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios</papertitle>
              </a>
              <br>
               <strong> Sriram Mandalika </strong> and Athira Nambiar <br>
              <em>27th International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2024 <br>
               <p> In this work, we propose a novel active learning mechanism for semantic segmentation for self-driving vehicles using the Cityscapes dataset. We leverage the iterative data-pipeline to improve the model accuracy using the least amount labelled images along with a blend of XAI and entropy analysis. </p>
              </td>
          </tr> </tbody></table>


		  
         
   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/bookchapter1.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Deep Learning-Based Speech Emotional Analysis Using Convolution Neural Network: Bi-Directional Long Short-Term Memory</papertitle>
              </a>
              <br>
              S Aruna, G Usha, A Saranya, M Maheswari, <strong>MASS Mandalika </strong><br>
              <em><strong>IGI Global</strong></em>, 2024 <br>
               <p> This book chapter, titled "<a href="https://www.igi-global.com/book/machine-deep-learning-techniques-emotion/338317">Machine and Deep Learning Techniques for Emotion Detection</a>," delves into the application of deep learning (DL) as part of artificial intelligence (AI) to perform tasks requiring human intelligence. Specifically, it focuses on speech emotion recognition (SpEmRe), which identifies various emotions from audio samples. DL techniques, especially bi-directional long short-term memory (Bi-LSTM) models, are utilized to predict human emotions from speech, reflecting the growing interest.. <a href="https://www.igi-global.com/book/machine-deep-learning-techniques-emotion/338317">Read more</a> </p>
              </td>
          </tr> </tbody></table>
   
         <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CVPR_2021_TCL.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href=https://arxiv.org/pdf/2102.02751.pdf>
                <papertitle>Semi-Supervised Action Recognition with Temporal Contrastive Learning</papertitle>
              </a>
              <br>
               <strong>Ankit Singh<span>&#42;</span> </strong>, Omprakash Chakraborty<span>&#42;</span>, Ashutosh Varshney, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das<br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021 <br>
               <p>We propose a temporal contrastive learning framework for semi-supervised action recognition by using contrastive losses between different videos and groups of videos with similar actions.</p>
              </td>
          </tr> </tbody></table> -->

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ECCV_2020_W.png" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href=data/ECCV_2020_W.pdf>
                <papertitle>Mitigating Dataset Imbalance via Joint Generation and Classification</papertitle>
              </a>
              <br>
              Aadarsh Sahoo<span>&#42;</span> , <strong>Ankit Singh<span>&#42;</span> </strong>, Rameswar panda, Rogerio Feris, Abir Das<br>
              <em>ECCV Workshop on Imbalance Problems in Computer Vision (<strong>ECCV-W</strong>)</em>, 2020
               <p>We introduce a joint dataset repairment strategy by combining classifier with a GAN that makes up for the deficit of training examples from the minority class by producing additional examples.</p>
              </td>
          </tr>
           </tbody></table> -->
   <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr> <td style="padding:20px;width:100%;vertical-align:middle"> -->
                 
              <heading>Services</heading>
              <ul>
                     <li> <strong>Reviewer</strong>: ICML<br>
              </ul>
                
            </td> </tr>
        </tbody></table> -


	      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info/">here </a>
                </p>
               <p style="text-align:left;font-size:small;">
                <span>&#42;</span> denotes equal contribution
                </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>
</html>
