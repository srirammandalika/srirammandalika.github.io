<!DOCTYPE HTML>
<link rel="shortcut icon" type="image/png" href="./images/MLogo.png" />
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>Sriram Mandalika</title>

  <meta name="author" content="Sriram Mandalika">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Sriram Mandalika",
  "url": "https://srirammandalika.github.io/",
  "sameAs": [
    "https://www.wikidata.org/wiki/Q135488403",
    "https://www.linkedin.com/in/srirammandalika/",
    "https://scholar.google.com/citations?user=7DQaLdgAAAAJ",
    "https://orcid.org/0009-0009-8390-274X"
  ],
  "jobTitle": "Student Researcher",
  "worksFor": {
    "@type": "Organization",
    "name": "Hasso Plattner Institute (HPI)", "University of Potsdam"
  }
}
</script>
 



</head>


<!-- <header>
  <nav class="site-nav">
    <ul>
      <li><a href="#about">About</a></li>
      <li><a href="#research">Research</a></li>
      <li><a href="#publications">Publications</a></li>
      <li><a href="#blog">Blog</a></li>
    </ul>
  </nav>
</header> -->



<body>

  <!-- FULL‑WIDTH NAV -->
      <!-- <nav class="site-nav">
    <div class="nav-inner container">
      <ul>
        <li><a href="#about">About</a></li>
        <li><a href="#research">Research</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#blog">Blog</a></li>
      </ul>
    </div>
  </nav> -->




  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sriram Mandalika</name><br>
                <designation>MSc. Student at University of Potsdam & HPI</designation>
              </p>
              

              <p>
                Hello! I am currently pursuing a Master's degree in Computer Science at the <a href="https://hpi.de/en/"> Hasso Plattner Institute (HPI)</a>, <a href="https://www.uni-potsdam.de/en/university-of-potsdam">University of Potsdam</a>.  <br>
              </p>
              <p>
                I previously graduated with a Bachelor's of Technology (B.Tech) from <a href="https://www.srmist.edu.in/">SRM Institute of Science and Technology, Chennai (SRM IST)</a> supervised by <a href="https://www.srmist.edu.in/faculty/dr-athira-m-nambiar/"> Prof. Dr. Athira M. Nambiar</a> at <a href="https://sites.google.com/srmist.edu.in/webpages/home">Center for AI in Computer Vision</a>, where I focused on vision models, learning methodologies and reasoning.
              </p>
              <p>
                My main research interest lie in the areas of computer vision and deep learning. Particularly focused on learning methods for vision models, label-efficient (Semi-Supervised/ Unsupervised /Self-Supervised ) approaches that can independently perform iterative self-improvement and world models. However, I am always open to new ideas and collaborations in related fields. This website gives you an overview of my recent research and history.
              </p>


               <p style="text-align:center">
                <a href="mailto:sriam.mandalika@uni-potsdam.de">Email</a> &nbsp/&nbsp
                 <a href="./Data/Sriram_EU_CV.pdf">CV</a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/srirammandalika/">LinkedIn</a>&nbsp/&nbsp
                  <a style="text-align:center"></a>
               <a href="https://scholar.google.co.in/citations?hl=en&user=7DQaLdgAAAAJ">Google Scholar</a> &nbsp/&nbsp
               <a href="https://github.com/srirammandalika"> Github </a> 
              </p>
            </td>

            <!-- <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./images/Sriram.png"><img style="width:90%;max-width:90%" alt="profile photo" src="./images/Sriram.png" class="hoverZoomLink"></a>
            </td> -->


            <td style="padding:2.5%; width:40%; max-width:40%;">
  <a href="./images/Sriram_web.jpeg">
    <img
      src="./images/Sriram_web.jpeg"
      alt="profile photo"
      style="
        width: 90%;
        aspect-ratio: 1/1;
        max-width: 90%;
        object-position: left;
        object-fit: cover;
        border-radius: 50%;
        display: block;
        margin-left: auto;
      "
      class="hoverZoomLink"
    />
  </a>
</td>





          </tr>
        </tbody></table>



		  <div style="display: flex; justify-content: center; align-items: center; width: 100%; padding: 20px; box-sizing: border-box;">
          <div style="flex: 1; text-align: center;">
            <a href="https://www.srmist.edu.in/" rel="external nofollow noopener" target="_blank">
              <img src="/images/srm.png" style="width: 100px;" alt="SRM Institute of Science and Technology logo" />
            </a>
          </div>
          <div style="flex: 1; text-align: center;">
            <a href="https://ethz.ch/en.html" rel="external nofollow noopener" target="_blank">
              <img src="/images/Potsdam.png" style="width: 120px;" alt="University of Potsdam" />
            </a>
          </div>
          <div style="flex: 1; text-align: center;">
            <a href="https://www.tum.de/en/" rel="external nofollow noopener" target="_blank">
              <img src="/images/HPI.svg" style="width: 150px;" alt="Hasso Plattner Institute" />
            </a>
          </div>
          <!-- <div style="flex: 1; text-align: center;">
            <a href="https://www.polytechnique.edu/en" rel="external nofollow noopener" target="_blank">
              <img src="/assets/img/logos/lx_logo.png" style="width: 60px;" alt="École Polytechnique logo, stylized X symbol, shown with other research institution logos in a calm and professional context" />
            </a>
          </div> -->
          <!-- <div style="flex: 1; text-align: center;">
            <a href="https://www.ku.dk/english/" rel="external nofollow noopener" target="_blank">
              <img src="/assets/img/logos/ucph_logo.png" style="width: 60px;" alt="University of Copenhagen logo, circular emblem with Latin text, presented alongside other university logos in a collaborative academic environment" />
            </a>
          </div> -->
        </div>

		  


          <!-- <div style="display: flex; justify-content: center; align-items: center; width: 100%; padding: 20px; box-sizing: border-box;"> <div style="flex: 1; text-align: center;"> <a href="" rel="external nofollow noopener" target="_blank"> <img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExaG9nMjljNTNwMG96djl5eHVrOG1xamZ3Y2ZuMXBzZ25xcm9zd3Q4bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/iHD88spVFkL7mZakwa/giphy.gif" style="width: 60px;"> </a> </div> <div style="flex: 1; text-align: center;"> <a href="https://www.nrsc.gov.in/" rel="external nofollow noopener" target="_blank"> <img src="images/isro copy.png" style="width: 90px;"> </a> </div> <div style="flex: 1; text-align: center;"> <a href="https://www.tum.de/en/" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/logos/tum_logo.jpg" style="width: 60px;"> </a> </div> <div style="flex: 1; text-align: center;"> <a href="https://www.polytechnique.edu/en" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/logos/lx_logo.png" style="width: 60px;"> </a> </div> <div style="flex: 1; text-align: center;"> <a href="https://www.ku.dk/english/" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/logos/ucph_logo.png" style="width: 60px;"> </a> 
          </div> 
        </div> -->


        <!-- News -->
        <table style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="width:100%;vertical-align:middle">
                <heading>News</heading>
              </td>
            </tr>
        </tbody></table>
        <table style="width:95%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
          <!-- ITEM -->
          <tr>
            <td>
              <p style="display:inline"><b>08/2025</b></p>
            </td>
              <td>
                Our Paper on Unsupervised Continual Learning got selected for <i style="color:red;"><strong>Spotlight Presentation</strong></i> at <a href="https://ecai2025.org/"> ECAI 2025 </a>.
              </td>
          </tr>
         
         
          <!-- ITEM -->
          <tr>
            <td>
              <p style="display:inline"><b>06/2025</b></p>
            </td>
              <td>
                Paper on Unsupervised Continual Learning accepted at <a href="https://ecai2025.org/"> ECAI 2025 </a>, <a href="https://arxiv.org/abs/2505.04787"> arxiv</a>.
              </td>
          </tr>
          <!-- ITEM -->
          <tr>
            <td>
              <p style="display:inline"><b>05/2025</b></p>
            </td>
              <td>
                Received Outstanding Research Contribution Award 2025, SRM IST! 
              </td>
          </tr>
          <!-- ITEM -->
          <tr>
            <td>
              <p style="display:inline"><b>04/2025</b></p>
            </td>
              <td>
                Paper on Precognitive Uncertainty-aware Chain-of-Thought (CoT) accepted at <a href="https://cvpr.thecvf.com/"> CVPR Precognition Workshop 2025 </a>, <a href="https://arxiv.org/abs/2504.05908"> arxiv</a> 
              </td>
          </tr>
          <!-- ITEM -->
           <tr>
            <td>
              <p style="display:inline"><b>09/2024</b></p>
            </td>
              <td>
                Our work on <a href="https://arxiv.org/abs/2408.04482">Active Learning</a> is accepted at <a href="https://bmvc2023.org/">ICPR'24</a>! 
              </td>
          </tr> 
        </tbody></table>



        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:center">
              <heading> News </heading>
              <ul>
                <li> <strong>[07/2025] </strong> Paper on Unsupervised Continual Learning accepted at <a href="https://ecai2025.org/"> ECAI 2025 </a>, <a href="https://arxiv.org/abs/2505.04787"> arxiv</a></li>
                <li> <strong>[05/2025] </strong> Received Outstanding Research Contribution Award 2025, SRM IST</li>
                <li> <strong>[07/2025] </strong> Paper on Precognitive Uncertainty-aware Chain-of-Thought (CoT) for driving scene scenario accepted at <a href="https://cvpr.thecvf.com/"> CVPR Precognition Workshop 2025 </a>, <a href="https://arxiv.org/abs/2504.05908"> arxiv</a></li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->



	      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
        <hr>

        I’ve worked with some amazing people, especially (in no particular order) <a href="https://nambiarathira.blogspot.com/search/label/Home/">Dr. Athira Nambiar</a>, <a href="https://people.iith.ac.in/ckm/">Prof. C. Krishna Mohan</a>, <a href="https://www.linkedin.com/in/mrinmay-sen-ab8ba1176/?originalSubdomain=in">Mr. Mrinmay Sen</a>, <a href="https://www.linkedin.com/in/rahul-biswas-5a2b1a147/">Mr. Rahul Biswas</a>, <a href="https://www.linkedin.com/in/saiveena-suresh-a37705277/?originalSubdomain=in">Ms. Sai Veena Suresh</a>, <a href="https://www.linkedin.com/in/sampath-kumar-a23319251/">Mr. Sampath Kumar</a>, and <a href="https://www.linkedin.com/in/shilpi-garg-399180155/">Ms. Shilpi Garg</a> for supporting my research journey.

	<hr> -->

	      
        


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
               <heading>Highlights</heading>
              <ul>
		 <li> Paper on Unsupervised Continual Learning accepted at <a href="https://ecai2025.org/"> ECAI 2025 </a>, <a href="https://arxiv.org/abs/2505.04787"> arxiv</a><br>
		 <li> Received Outstanding Research Contribution Award 2025, SRM IST</li>     
		 <li> Paper on Precognitive Uncertainty-aware Chain-of-Thought (CoT) for driving scene scenario accepted at <a href="https://cvpr.thecvf.com/"> CVPR 2025 </a>, <a href="https://arxiv.org/abs/2504.05908"> arxiv</a><br>
                 <li> Paper on Semi-Supervised semantic segmentation for driving scene scenario accepted at <a href="https://link.springer.com/chapter/10.1007/978-3-031-78107-0_8"> ICPR 2024 </a>, <a href="https://arxiv.org/abs/2408.04482"> arxiv</a> <br>
                 <li> Book chapter on Speech emotion analysis <a href=https://www.igi-global.com/chapter/deep-learning-based-speech-emotional-analysis-using-convolution-neural-network/347293> IGI Global</a> <br> 
                 <li> Institute Research Award 2022, <a href=https://www.iitm.ac.in/> IIT Madras </a> <br>
                 <li> Student research collaborator at <a href="https://ckmvigil.in/"> IIT Hyderabad</a>, explored Continual Learning and Image Reconstruction methods. <br> 
                  <li> Paper on Semi-Supervised Domain Adaptation accepted at <a href=https://nips.cc> NeurIPS 2021</a> <br>
                 <li> Paper on Semi-Supervised Action Recognition accepted at <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.<br>
              <li> Paper on Mitigating data Imbalance accepted at <a href="https://eccv2020.eu/">ECCV-W 2020</a>.<br> 
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table> -->
<!--     <p> Selected publications are listed here; for the full list of works, kindly visit the Google Scholar link provided above. </p> -->
		
		  


		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>For the up-to-date publication list, please visit the <a href="https://scholar.google.co.in/citations?hl=en&user=7DQaLdgAAAAJ">Google Scholar</a> page.</p> <be></be>
            




               <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/Arch.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay</papertitle>
              </a>
              <br>
              <strong>Sriram Mandalika </strong>, Jammigumpula Harsha Vardhan, Athira Nambiar 
              <br>
              <em>28th European Conference on Artificial Intelligence <strong>(ECAI)</strong></em>, 2025 <br>
               <!-- <p> Replay to Remember (R2R) is an statistically driven generative replay framework under unsupervised incremental learning setting that employs generative replay to maintain performance on both past and incoming tasks. R2R dynamically balances real unlabeled data with VLM‑powered synthetic samples using cluster‑level uncertainty feedback and adaptive thresholding—operating without any pretraining or memory buffers and emulating biological memory replay to remember and act on new, unseen tasks. </p> -->
				<i style="color:red;"><strong>(Spotlight Paper)</strong></i>
                <p> <a href="https://www.arxiv.org/abs/2505.04787">Paper</a> | <a href="https://github.com/srirammandalika/R2R/">Code</a> | <a href="">Website</a>

                <br><br>
            </p>
              </td>
              





              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/precog_cvpr_25.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://srirammandalika.github.io/PRIMEDrive-CoT/">
                <papertitle>PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario</papertitle>
              </a>
              <br>
               <strong> Sriram Mandalika </strong>, Lalitha V and Athira Nambiar <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025 <br>
               <!-- <p> In this work, we introduce PRIMEDrive-CoT, an uncertainty-aware model for object interaction and Chain-of-Thought (CoT) reasoning in driving scenarios. It integrates LiDAR-based 3D detection with multi-view RGB for interpretable scene understanding. Bayesian GNNs model uncertainty and interactions, while CoT reasoning and Grad-CAM enhance interpretability. Evaluated on DriveCoT, PRIMEDrive-CoT outperforms existing CoT and risk-aware models.</p> -->
				
              <p> <a href="https://arxiv.org/abs/2504.05908">Paper</a> | <a href="https://github.com/srirammandalika/Precognition-CVPR-2025">Code</a> | <a href="VITAL-Project/">Website</a>

                <br><br>
            </p>
              </td>



              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/XAL_Cityscape_new.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios</papertitle>
              </a>
              <br>
               <strong> Sriram Mandalika </strong> and Athira Nambiar <br>
              <em>27th International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2024 <br>
               <!-- <p> In this work, we propose a novel active learning mechanism for semantic segmentation for self-driving vehicles using the Cityscapes dataset. We leverage the iterative data-pipeline to improve the model accuracy using the least amount labelled images along with a blend of XAI and entropy analysis. </p> -->
                <p> <a href="https://arxiv.org/abs/2408.04482">Paper</a> | <a href="https://github.com/srirammandalika/SegXAL-ICPR_2024">Code</a> | <a href="">Website</a>

                <br><br>
            </p>
              </td>






















       
            
            </td>

          </tr>
        </tbody></table>
		<!-- <div class="pub-note">* Equal contribution.&nbsp;&nbsp;† Equal advising.</div> <br> -->



   
         <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CVPR_2021_TCL.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href=https://arxiv.org/pdf/2102.02751.pdf>
                <papertitle>Semi-Supervised Action Recognition with Temporal Contrastive Learning</papertitle>
              </a>
              <br>
               <strong>Ankit Singh<span>&#42;</span> </strong>, Omprakash Chakraborty<span>&#42;</span>, Ashutosh Varshney, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das<br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021 <br>
               <p>We propose a temporal contrastive learning framework for semi-supervised action recognition by using contrastive losses between different videos and groups of videos with similar actions.</p>
              </td>
          </tr> </tbody></table> -->

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ECCV_2020_W.png" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href=data/ECCV_2020_W.pdf>
                <papertitle>Mitigating Dataset Imbalance via Joint Generation and Classification</papertitle>
              </a>
              <br>
              Aadarsh Sahoo<span>&#42;</span> , <strong>Ankit Singh<span>&#42;</span> </strong>, Rameswar panda, Rogerio Feris, Abir Das<br>
              <em>ECCV Workshop on Imbalance Problems in Computer Vision (<strong>ECCV-W</strong>)</em>, 2020
               <p>We introduce a joint dataset repairment strategy by combining classifier with a GAN that makes up for the deficit of training examples from the minority class by producing additional examples.</p>
              </td>
          </tr>
           </tbody></table> -->
   <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr> <td style="padding:20px;width:100%;vertical-align:middle"> -->
                 




        <!-- <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Service</heading>
            <p style="text-align:justify">
                <b>Reviewer:</b> TMLR, ICCV (MCL Workshop 2025, XAI Workshop 2025), ICML 2025, ICML LatinX Workshop 2025, BMVC 2025.
            </p>
          </td>
        </tr> -->





        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Service</heading>
            <p style="text-align:justify">
                <b>Reviewer:</b> TMLR, ICCV (MCL Workshop 2025, XAI Workshop 2025), ICML 2025, ICML LatinX Workshop 2025, BMVC 2025.
            </p>
          </td>
        </tr>

      </tbody>
</table>


	      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <a href="https://visitorbadge.io/status?path=https%3A%2F%2Fsrirammandalika.github.io%2F"><img src="https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fsrirammandalika.github.io%2F&countColor=%23263759" /></a>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info/">here </a>
                </p>
               <p style="text-align:left;font-size:small;">
                <span>&#42;</span> denotes equal contribution
                </p>
            </td>


			  <a href='https://mapmyvisitors.com/web/1bz29'  title='Visit tracker'><img src='https://mapmyvisitors.com/map.png?cl=ffffff&w=300&t=n&d=DX15I5ozLq5Q-wR0ekcNB17qazZ99Mm2sOgYD9FXvrM&co=2d78ad&ct=ffffff'/></a>
			  
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</div>
</body>



</html>
